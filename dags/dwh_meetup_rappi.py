"""
dwh_meetup_rappi
DAG auto-generated by Astro Cloud IDE.
"""

from airflow.decorators import dag
from astro import sql as aql
import pandas as pd
import pendulum

import os
import logging
from kaggle.api.kaggle_api_extended import KaggleApi
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.hooks.base import BaseHook
import zipfile
import tempfile

@aql.dataframe(task_id="download_files")
def download_files_func():
    
    def upload_files_to_snowflake(files_list, stage_name):
        try:
            # Obtener la conexión a Snowflake desde Airflow
            conn = BaseHook.get_connection('cnn_snow_rappi_stage')
            hook = conn.get_hook()
    
            # Subir cada archivo al stage en Snowflake
            for file_path in files_list:
                logging.info(f"Subiendo el archivo {file_path} a Snowflake en el stage {stage_name}")
    
                # Construir el comando PUT para Snowflake
                sql = f"PUT 'file://{file_path}' @{stage_name} AUTO_COMPRESS=FALSE"
    
                # Ejecutar el comando PUT en Snowflake
                hook.run(sql)
    
                logging.info(f"El archivo {file_path} se subió correctamente a Snowflake en el stage {stage_name}")
    
                # Eliminar el archivo temporal después de la subida
                os.remove(file_path)
                logging.info(f"El archivo temporal {file_path} ha sido eliminado después de la carga.")
    
        except Exception as e:
            logging.error(f"Error al subir los archivos a Snowflake: {e}")
            raise  # Esto detendrá el proceso y permitirá que Airflow gestione el error
    
    # Ejemplo de uso
    
    def download_and_extract_files(dataset, download_path='/tmp', stage_name='stage_test_rappi/DATA_INPUT'):
        try:
            # Configuración de logging
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
            # Autenticación con la API de Kaggle
            api = KaggleApi()
            api.authenticate()
    
            # Listar todos los archivos del dataset
            files = api.dataset_list_files(dataset).files
            logging.info(f"Archivos encontrados en el dataset {dataset}: {[file.name for file in files]}")
    
            # Descargar y descomprimir cada archivo
            for file in files:
                file_name = file.name
                
                # Excluir el archivo members.csv
                if file_name == 'members.csv':
                    logging.info(f"El archivo {file_name} ha sido excluido de la descarga.")
                    continue
    
                logging.info(f"Descargando el archivo {file_name} desde el dataset {dataset}")
                api.dataset_download_file(dataset, file_name, path=download_path)
    
                # Verificar si el archivo descargado es un ZIP
                zip_file_path = os.path.join(download_path, f"{file_name}.zip")
                csv_file_path = os.path.join(download_path, file_name)
    
                if os.path.exists(zip_file_path):
                    logging.info(f"Descomprimiendo el archivo {zip_file_path}")
                    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                        zip_ref.extractall(download_path)
                    logging.info(f"El archivo {zip_file_path} se descomprimió correctamente.")
    
                    # Eliminar el archivo ZIP después de la descompresión
                    os.remove(zip_file_path)
                    logging.info(f"El archivo ZIP {zip_file_path} se eliminó después de descomprimirlo.")
    
                    # Subir el archivo CSV descomprimido a Snowflake
                    upload_files_to_snowflake([csv_file_path], stage_name)
    
                # Si ya se descargó como CSV directamente
                elif os.path.exists(csv_file_path):
                    logging.info(f"El archivo {file_name} se descargó correctamente en {csv_file_path}")
                    
                    # Subir el archivo CSV descargado a Snowflake
                    upload_files_to_snowflake([csv_file_path], stage_name)
    
                else:
                    raise FileNotFoundError(f"El archivo {file_name} no se descargó correctamente.")
    
            logging.info("Todos los archivos del dataset se han descargado, descomprimido y subido a Snowflake correctamente.")
    
        except Exception as e:
            logging.error(f"Error al descargar, descomprimir o subir los archivos del dataset {dataset} a Snowflake: {e}")
            raise
    
    download_and_extract_files('megelon/meetup')
    

@aql.dataframe(task_id="exe_sp_load_stage_data")
def exe_sp_load_stage_data_func():
    conn = BaseHook.get_connection('cnn_snow_rappi_stage')
    sql="CALL LOAD_STAGE_DATA();"
    hook = conn.get_hook()
    hook.run(sql)

@aql.dataframe(task_id="unload_bucket_s3")
def unload_bucket_s3_func():
    snowflake_hook = SnowflakeHook(snowflake_conn_id='cnn_snow_rappi')
    s3_hook = S3Hook(aws_conn_id='cnn_s3_bucket_rappi')
    conn = snowflake_hook.get_conn()
    cursor = conn.cursor()
    
    # Obtener las credenciales temporales desde S3Hook
    credentials = s3_hook.get_credentials()
    aws_access_key_id = credentials.access_key
    aws_secret_access_key = credentials.secret_key
    aws_session_token = credentials.token
    
    # Define el SQL de UNLOAD para Snowflake
    unload_sql = f"""
    COPY INTO 's3://buckets3-rappi-test/data_output/dim_categoria.csv'
    FROM (
        SELECT * FROM DIM_CATEGORIES
    )
    CREDENTIALS = (
        AWS_KEY_ID='{aws_access_key_id}'
        AWS_SECRET_KEY='{aws_secret_access_key}'
        
    )
    FILE_FORMAT = (FORMAT_NAME = 's3_csv_format')
    HEADER = TRUE
    OVERWRITE = TRUE;
    """
    
    try:
        cursor.execute(unload_sql)
        logging.info("Data successfully unloaded to S3")
    except Exception as e:
        logging.error(f"Error while unloading data to S3: {e}")
    finally:
        cursor.close()
        conn.close()

default_args={
    "owner": "oscar andres morales perez,Open in Cloud IDE",
}

@dag(
    default_args=default_args,
    schedule="0 0 * * *",
    start_date=pendulum.from_format("2024-08-27", "YYYY-MM-DD").in_tz("UTC"),
    catchup=False,
    owner_links={
        "oscar andres morales perez": "mailto:omoralesperez@gmail.com",
        "Open in Cloud IDE": "https://cloud.astronomer.io/cm06zmrq309x401n6211h3ngz/cloud-ide/cm0b3z9670sry01n5ylkjp5ca/cm0cd5wv80zqu01n57sdg733k",
    },
)
def dwh_meetup_rappi():
    download_files = download_files_func()

    exe_sp_load_stage_data = exe_sp_load_stage_data_func()

    unload_bucket_s3 = unload_bucket_s3_func()

    exe_sp_load_stage_data << download_files

    unload_bucket_s3 << exe_sp_load_stage_data

dag_obj = dwh_meetup_rappi()
